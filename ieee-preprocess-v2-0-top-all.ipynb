{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"run_checks = False\nrun_sample = False","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6240d42e-a73e-4104-9076-71a4f5f4f732","_cell_guid":"e1aad90f-aee7-428a-b692-0c13cb713e28","trusted":true},"cell_type":"markdown","source":"### Overview\nThis notebook works on the IEEE-CIS Fraud Detection competition. Here I build a simple XGBoost model based on a balanced dataset."},{"metadata":{},"cell_type":"markdown","source":"### Lessons:\n\n. keep the categorical variables as single items\n\n. Use a high max_depth for xgboost (maybe 40)\n\n\n### Ideas to try:\n\n. train divergence of expected value (eg. for TransactionAmt and distance based on the non-fraud subset (not all subset as in the case now)\n\n. try using a temporal approach to CV"},{"metadata":{"_uuid":"452e2475-e300-41b8-bbb6-ded3c1d99325","_cell_guid":"8d760d7d-53b1-4e67-877f-dce1ce151823","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# all imports necessary for this notebook\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport gc\nimport copy\nimport missingno as msno \nimport xgboost\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom sklearn.model_selection import StratifiedKFold, cross_validate, train_test_split \nfrom sklearn.metrics import roc_auc_score, r2_score\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d70cb9a7-e834-459c-8386-0200e7d3b25d","_cell_guid":"dbab167c-2ec4-481f-a561-6f01cbf288b2","trusted":true},"cell_type":"code","source":"# Helpers\n    \ndef seed_everything(seed=0):\n    '''Seed to make all processes deterministic '''\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    \ndef drop_correlated_cols(df, threshold, sample_frac = 1):\n    '''Drops one of two dataframe's columns whose pairwise pearson's correlation is above the provided threshold'''\n    if sample_frac != 1:\n        dataset = df.sample(frac = sample_frac).copy()\n    else:\n        dataset = df\n        \n    col_corr = set() # Set of all the names of deleted columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        if corr_matrix.columns[i] in col_corr:\n            continue\n        for j in range(i):\n            if (corr_matrix.iloc[i, j] >= threshold) and (corr_matrix.columns[j] not in col_corr):\n                colname = corr_matrix.columns[i] # getting the name of column\n                col_corr.add(colname)\n    del dataset\n    gc.collect()\n    df.drop(columns = col_corr, inplace = True)\n\ndef calc_feature_difference(df, feature_name, indep_features, min_r2 = 0.1, min_r2_improv = 0, frac1 = 0.1, \n                              max_depth_start = 2, max_depth_step = 4):\n    \n    from copy import deepcopy\n    \n    print(\"Feature name %s\" %feature_name)\n    #print(\"Indep_features %s\" %indep_features)\n    \n    is_imrpoving = True\n    curr_max_depth = max_depth_start\n    best_r2 = float(\"-inf\")\n    clf_best = np.nan\n    \n    while is_imrpoving:\n        clf = XGBRegressor(max_depth = curr_max_depth)\n\n        rand_sample_indeces = df[df[feature_name].notnull()].sample(frac = frac1).index\n        clf.fit(df.loc[rand_sample_indeces, indep_features], df.loc[rand_sample_indeces, feature_name]) \n\n        rand_sample_indeces = df[df[feature_name].notnull()].sample(frac = frac1).index\n        \n        pred_y = clf.predict(df.loc[rand_sample_indeces, indep_features])\n        r2Score = r2_score(df.loc[rand_sample_indeces, feature_name], pred_y)\n        print(\"%d, R2 score %.4f\" % (curr_max_depth, r2Score))\n        \n        curr_max_depth = curr_max_depth + max_depth_step\n        \n        if r2Score > best_r2:\n            best_r2 = r2Score\n            clf_best = deepcopy(clf)\n        if r2Score < best_r2 + (best_r2 * min_r2_improv) or (curr_max_depth > max_depth_start * max_depth_step and best_r2 < min_r2 / 2):\n            is_imrpoving = False\n\n    print(\"The best R2 score of %.4f\" % ( best_r2))\n    \n    if best_r2 > min_r2:\n        pred_feature = clf_best.predict(df.loc[:, indep_features])\n        return (df[feature_name] - pred_feature), best_r2\n    else:\n        return df[feature_name], best_r2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa9e0ef8-be7e-4968-92df-8fb00fc3babc","_cell_guid":"98d458c0-c733-461f-9954-21bc4d5cdfd2","trusted":true},"cell_type":"code","source":"seed_everything()\npd.set_option('display.max_columns', 500)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"master_df = pd.read_csv('/kaggle/input/ieee-preprocessed/master_df_top_all.csv')\nmaster_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_cat = {'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', \n            'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', \n            'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'ProductCD', 'card4', \n            'card6', 'M4','P_emaildomain',  'R_emaildomain', 'card1', 'card2', 'card3',  'card5', 'addr1', \n            'addr2', 'M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nindep_features = ['weekday', 'hours', 'TransactionDT', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5'\n                                                      , 'card6', 'addr1', 'addr2']\n\nfor feature in indep_features:\n    master_df[feature] = master_df[feature].astype('category').cat.codes\n\ncont_cols_list = list(master_df.select_dtypes(include='number').columns)\ncont_features_list = [x for x in cont_cols_list if x not in cols_cat and x not in ['TransactionID', 'isFraud', 'TransactionDT', 'is_train_df']]\n\nfor cont_feature in cont_features_list:\n    print(cont_feature)\n    master_df[cont_feature], best_r2 = calc_feature_difference(master_df, cont_feature, indep_features, frac1= 0.025)\n    if best_r2 > 0.9:\n        master_df.drop(columns = [cont_feature], inplace = True)\n    print(80 * '-')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"master_df.to_csv('master_df_time_adjusted_top_all.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}